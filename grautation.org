* 训练误差和泛化误差

训练误差（training error）是指，模型在训练数据集上计算得到的误差。
泛化误差（generalization error）是指，模型应⽤在同样从原始样本的分布中
抽取的⽆限多数据样本时，模型误差的期望。

* 模型复性杂

当我们有简单的模型和⼤量的数据时，我们期望泛化误差与训练误差相近。当我们有更复杂的模型和更少的
样本时，我们预计训练误差会下降，但泛化误差会增⼤。

* 模型选择
在机器学习中，我们通常在评估⼏个候选模型后选择最终的模型。这个过程叫做模型选择。有时，需要进⾏
⽐较的模型在本质上是完全不同的（⽐如，决策树与线性模型）
。⼜有时，我们需要⽐较不同的超参数设置下
的同⼀类模型。
例如，训练多层感知机模型时，我们可能希望⽐较具有不同数量的隐藏层、不同数量的隐藏单元以及不同的
的激活函数组合的模型。为了确定候选模型中的最佳模型，我们通常会使⽤验证集。

** 验证集
   解决此问题的常⻅做法是将我们的数据分成三份，除了训练和测试数据集之外，还增加⼀个验证数据集
（validation dataset），也叫验证集（validation set）。
*** K折交叉验证
    当训练数据稀缺时，我们甚⾄可能⽆法提供⾜够的数据来构成⼀个合适的验证集。这个问题的⼀个流⾏的解
决⽅案是采⽤K折交叉验证。这⾥，原始训练数据被分成K个不重叠的⼦集。然后执⾏K次模型训练和验证，
每次在K − 1个⼦集上进⾏训练，并在剩余的⼀个⼦集（在该轮中没有⽤于训练的⼦集）上进⾏验证。最后，
通过对K次实验的结果取平均来估计训练和验证误差。

* ⽋拟合还是过拟合
  训练和验证误差之间的泛化误差很⼩，我们有
理由相信可以⽤⼀个更复杂的模型降低训练误差。这种现象被称为⽋拟合（underfitting）。
  训练误差明显低于验证误差时要⼩⼼，这表明严重的过拟合（overfitting）。
  将模型在训练数据上拟合的⽐在潜在分布中更接近的现象称为过拟合（overfitting），⽤于对抗过拟合的技术称为正则化（regularization）。

* 前向传播
前向传播（forward propagation或forward pass）指的是：按顺序（从输⼊层到输出层）计算和存储神经⽹
络中每层的结果.
* 反向传播
反向传播（backward propagation或backpropagation）指的是计算神经⽹络参数梯度的⽅法。
该⽅
法根据微积分中的链式规则，按相反的顺序从输出层到输⼊层遍历⽹络。该算法存储了计算某些参数梯度时
所需的任何中间变量（偏导数）。

** Pages
P163

* 训练神经⽹络
在训练神经⽹络时，前向传播和反向传播相互依赖。对于前向传播，我们沿着依赖的⽅向遍历计算图并计算
其路径上的所有变量。然后将这些⽤于反向传播，其中计算顺序与计算图的相反。

因此，在训练神经⽹络时，在初始化模型参数后，我们交替使⽤前向传播和反向传播，利⽤反向传播给出的
梯度来更新模型参数。注意，反向传播重复利⽤前向传播中存储的中间值，以避免重复计算。带来的影响之
⼀是我们需要保留中间值，直到反向传播完成。这也是训练⽐单纯的预测需要更多的内存（显存）的原因之
⼀。此外，这些中间值的⼤⼩与⽹络层的数量和批量的⼤⼩⼤致成正⽐。因此，使⽤更⼤的批量来训练更深
层次的⽹络更容易导致内存不⾜（out of memory）错误。

* 分布偏移的类型
** 协变量偏移
假设：虽然输⼊的分布可能随时间⽽改变，
但标签函数（即条件分布P (y | x)）没有改变。统计学家称之为协变量偏移（covariate shift）.

** 标签偏移
标签偏移（label shift）描述了与协变量偏移相反的问题。这⾥我们假设标签边缘概率P (y)可以改变，但是类
别条件分布P (x | y)在不同的领域之间保持不变。当我们认为y导致x时，标签偏移是⼀个合理的假设。

* 概念偏移
我们也可能会遇到概念偏移（concept shift）：当标签的定义发⽣变化时，就会出现这种问题。


* 层
（1）接受⼀组输⼊，
（2）⽣成相应的输出，
（3）由⼀组可调整参数描述。
* 块
块（block)可以描述单个层、由多个层组成的组件或整个模型本⾝。
* 循环神经⽹络
** 序列模型
*** 统计⼯具
**** ⾃回归模型
⾃回归模型（autoregressivemodels）
隐变量⾃回归模型（latent autoregressive models）
**** ⻢尔可夫模型
在⾃回归模型的近似法中，我们使⽤x t−1 , . . . , x t−τ ⽽不是x t−1 , . . . , x 1 来估计x t 。只要这种是近似
精确的，我们就说序列满⾜⻢尔可夫条件（Markov condition）。特别是，如果τ = 1，得到⼀个 ⼀阶⻢尔可夫模型（first-order Markov model）
**** 因果关系

** 多层感知机模型    P311
*** ⽆隐状态的神经⽹络
*** 有隐状态的循环神经⽹络
从相邻时间步的隐藏变量H t 和 H t−1 之间的关系可知，这些变量捕获并保留了序列直到其当前时间步的历史信息，就如当前时间步下神经⽹络的状
态或记忆，因此这样的隐藏变量被称为隐状态（hidden state）由于在当前时间步中，隐状态使⽤的定义与
前⼀个时间步中使⽤的定义相同，因此计算是循环的（recurrent）。
于是基于循环计算的隐状态神经⽹络被命名为 循环神经⽹络（recurrent neural network）。
在循环神经⽹络中执⾏H t = ϕ(X t W xh + H t−1 W hh + b h )计算的层称为循环层（recurrent layer）。

循环神经⽹络的参数包括隐藏层的权重 W xh ∈ R d×h , W hh ∈ R h×h 和偏置b h ∈ R 1×h ，以及输出层的权
重W hq ∈ R h×q 和偏置b q ∈ R 1×q 。值得⼀提的是，即使在不同的时间步，循环神经⽹络也总是使⽤这些模型
参数。因此，循环神经⽹络的参数开销不会随着时间步的增加⽽增加。

沿列（轴1）
沿⾏（轴0）
#+begin_src python : Eg
torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))
#+end_src

** 梯度裁剪
P318   D2l

* ⻔控循环单元（GRU）
⻔控循环单元与普通的循环神经⽹络之间的关键区别在于：后者⽀持隐状态的⻔控。这意味着模型有专⻔的
机制来确定应该何时更新隐状态，以及应该何时重置隐状态。

** ⻔控隐状态
*** 重置⻔和更新⻔ P334 D2l


重置⻔（reset gate）和更新⻔（update gate）。
⻔控循环单元具有以下两个显著特征：
• 重置⻔有助于捕获序列中的短期依赖关系。
• 更新⻔有助于捕获序列中的⻓期依赖关系。

我们把它们设计成(0, 1)区间中的向量，这样我们就可以进⾏凸组合。

重置⻔允许我们控制“可能还想记住”的过去状态的数量；
更新⻔将允许我们控制新状态中有多少个是旧状态的副本。

