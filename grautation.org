
* 训练误差和泛化误差

训练误差（training error）是指，模型在训练数据集上计算得到的误差。
泛化误差（generalization error）是指，模型应⽤在同样从原始样本的分布中
抽取的⽆限多数据样本时，模型误差的期望。

* 模型复性杂

当我们有简单的模型和⼤量的数据时，我们期望泛化误差与训练误差相近。当我们有更复杂的模型和更少的
样本时，我们预计训练误差会下降，但泛化误差会增⼤。


* 模型选择
在机器学习中，我们通常在评估⼏个候选模型后选择最终的模型。这个过程叫做模型选择。有时，需要进⾏
⽐较的模型在本质上是完全不同的（⽐如，决策树与线性模型）
。⼜有时，我们需要⽐较不同的超参数设置下
的同⼀类模型。
例如，训练多层感知机模型时，我们可能希望⽐较具有不同数量的隐藏层、不同数量的隐藏单元以及不同的
的激活函数组合的模型。为了确定候选模型中的最佳模型，我们通常会使⽤验证集。

** 验证集
   解决此问题的常⻅做法是将我们的数据分成三份，除了训练和测试数据集之外，还增加⼀个验证数据集
（validation dataset），也叫验证集（validation set）。
*** K折交叉验证
    当训练数据稀缺时，我们甚⾄可能⽆法提供⾜够的数据来构成⼀个合适的验证集。这个问题的⼀个流⾏的解
决⽅案是采⽤K折交叉验证。这⾥，原始训练数据被分成K个不重叠的⼦集。然后执⾏K次模型训练和验证，
每次在K − 1个⼦集上进⾏训练，并在剩余的⼀个⼦集（在该轮中没有⽤于训练的⼦集）上进⾏验证。最后，
通过对K次实验的结果取平均来估计训练和验证误差。



* ⽋拟合还是过拟合
  训练和验证误差之间的泛化误差很⼩，我们有
理由相信可以⽤⼀个更复杂的模型降低训练误差。这种现象被称为⽋拟合（underfitting）。
  训练误差明显低于验证误差时要⼩⼼，这表明严重的过拟合（overfitting）。
  将模型在训练数据上拟合的⽐在潜在分布中更接近的现象称为过拟合（overfitting），⽤于对抗过拟合的技术称为正则化（regularization）。

* 前向传播
前向传播（forward propagation或forward pass）指的是：按顺序（从输⼊层到输出层）计算和存储神经⽹
络中每层的结果.

* 反向传播
反向传播（backward propagation或backpropagation）指的是计算神经⽹络参数梯度的⽅法。
该⽅
法根据微积分中的链式规则，按相反的顺序从输出层到输⼊层遍历⽹络。该算法存储了计算某些参数梯度时
所需的任何中间变量（偏导数）。

** Pages
P163


* 训练神经⽹络
在训练神经⽹络时，前向传播和反向传播相互依赖。对于前向传播，我们沿着依赖的⽅向遍历计算图并计算
其路径上的所有变量。然后将这些⽤于反向传播，其中计算顺序与计算图的相反。

因此，在训练神经⽹络时，在初始化模型参数后，我们交替使⽤前向传播和反向传播，利⽤反向传播给出的
梯度来更新模型参数。注意，反向传播重复利⽤前向传播中存储的中间值，以避免重复计算。带来的影响之
⼀是我们需要保留中间值，直到反向传播完成。这也是训练⽐单纯的预测需要更多的内存（显存）的原因之
⼀。此外，这些中间值的⼤⼩与⽹络层的数量和批量的⼤⼩⼤致成正⽐。因此，使⽤更⼤的批量来训练更深
层次的⽹络更容易导致内存不⾜（out of memory）错误。

* 分布偏移的类型

** 协变量偏移
假设：虽然输⼊的分布可能随时间⽽改变，
但标签函数（即条件分布P (y | x)）没有改变。统计学家称之为协变量偏移（covariate shift）.

** 标签偏移
标签偏移（label shift）描述了与协变量偏移相反的问题。这⾥我们假设标签边缘概率P (y)可以改变，但是类
别条件分布P (x | y)在不同的领域之间保持不变。当我们认为y导致x时，标签偏移是⼀个合理的假设。

* 概念偏移
我们也可能会遇到概念偏移（concept shift）：当标签的定义发⽣变化时，就会出现这种问题。

| Name  |   id | Age |
|-------+------+-----|
| Peter | 1234 |  50 |
| Sue   | 4321 |  54 |



